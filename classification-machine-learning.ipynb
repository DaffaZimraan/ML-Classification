{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXfQ-XiHb28m"
      },
      "source": [
        "# Implementasi Model Klasifikasi - Machine Learning\n",
        "Berikut dataset yang akan digunakan untuk implementasi model klasifikasi:\n",
        "https://www.kaggle.com/datasets/uciml/iris\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fsBZ82BHe1H6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/DaffaZimraan/ML-Classification/main/IRIS_Train.csv')\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/DaffaZimraan/ML-Classification/main/IRIS_Test.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5XwuiBEefrk"
      },
      "source": [
        "## Decision Tree\n",
        "Decision tree adalah model prediksi yang menggunakan struktur pohon atau hierarki keputusan. Dalam decision tree, setiap simpul mewakili sebuah atribut atau fitur, setiap cabang mewakili sebuah keputusan atau aturan, dan setiap daun dari pohon mewakili hasil akhir. Model ini sering digunakan dalam analisis keputusan dan pembelajaran mesin untuk membantu mengidentifikasi strategi yang paling mungkin untuk mencapai tujuan tertentu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tG6-9r9mabp"
      },
      "source": [
        "### A. Menggunakan GINI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwqLJY3ic0L9"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdNjgm2ac90B",
        "outputId": "dcfd1604-5a6c-4f68-8619-149d3f6963e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9\n",
            "Precision: 0.923076923076923\n",
            "Recall: 0.9\n",
            "F1-Score: 0.911392405063291\n",
            "Specificity: 0.9500000000000001\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 3  7  0]\n",
            " [ 0  0 10]]\n"
          ]
        }
      ],
      "source": [
        "def calculate_metrics(conf_matrix):\n",
        "    FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix)\n",
        "    FN = conf_matrix.sum(axis=1) - np.diag(conf_matrix)\n",
        "    TP = np.diag(conf_matrix)\n",
        "    TN = conf_matrix.sum() - (FP + FN + TP)\n",
        "\n",
        "    precision = np.mean(TP / (TP + FP + np.finfo(float).eps))\n",
        "    recall = np.mean(TP / (TP + FN + np.finfo(float).eps))\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + np.finfo(float).eps)\n",
        "    specificity = np.mean(TN / (TN + FP + np.finfo(float).eps))\n",
        "\n",
        "    return precision, recall, f1_score, specificity\n",
        "\n",
        "def create_confusion_matrix(actual, predicted, classes):\n",
        "    class_dict = {cls: i for i, cls in enumerate(classes)}\n",
        "    matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
        "    for a, p in zip(actual, predicted):\n",
        "        matrix[class_dict[a]][class_dict[p]] += 1\n",
        "    return matrix\n",
        "\n",
        "def gini_index(groups, classes):\n",
        "    n_instances = float(sum([len(group) for group in groups]))\n",
        "    gini = 0.0\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] == class_val for row in group].count(True) / size\n",
        "            score += p * p\n",
        "        gini += (1.0 - score) * (size / n_instances)\n",
        "    return gini\n",
        "\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = list(), list()\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            gini = gini_index(groups, class_values)\n",
        "            if gini < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n",
        "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth+1)\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "train = train_data.values.tolist()\n",
        "test = test_data.values.tolist()\n",
        "\n",
        "tree = build_tree(train, max_depth=3, min_size=10)\n",
        "\n",
        "predictions = [predict(tree, row) for row in test]\n",
        "\n",
        "actual = [row[-1] for row in test]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJGmwEahfLBO"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsSffBemfeFN",
        "outputId": "b2ac950b-79f6-4041-e6bd-8eff967a4e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9\n",
            "Precision: 0.923076923076923\n",
            "Recall: 0.9\n",
            "F1-Score: 0.911392405063291\n",
            "Specificity: 0.9500000000000001\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 3  0  7]]\n"
          ]
        }
      ],
      "source": [
        "def min_max_normalize(dataset):\n",
        "    min_val = np.min(dataset, axis=0)\n",
        "    max_val = np.max(dataset, axis=0)\n",
        "    return (dataset - min_val) / (max_val - min_val + np.finfo(float).eps)\n",
        "\n",
        "train_values = train_data.values\n",
        "test_values = test_data.values\n",
        "train_normalized = min_max_normalize(train_values[:, :-1])\n",
        "train = np.column_stack((train_normalized, train_values[:, -1]))\n",
        "test_normalized = min_max_normalize(test_values[:, :-1])\n",
        "test = np.column_stack((test_normalized, test_values[:, -1]))\n",
        "\n",
        "tree = build_tree(train.tolist(), max_depth=3, min_size=10)\n",
        "\n",
        "predictions = [predict(tree, row[:-1]) for row in test.tolist()]\n",
        "\n",
        "actual = [row[-1] for row in test.tolist()]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijGtG7G1mg0q"
      },
      "source": [
        "### B. Menggunakan Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWM1weTEns-U"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CCFN10cnHef",
        "outputId": "87ae9e4c-5aaa-4663-e714-cf1ecc960f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9\n",
            "Precision: 0.923076923076923\n",
            "Recall: 0.9\n",
            "F1-Score: 0.911392405063291\n",
            "Specificity: 0.9500000000000001\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 3  0  7]]\n"
          ]
        }
      ],
      "source": [
        "def entropy(groups, classes):\n",
        "    total_instances = float(sum([len(group) for group in groups]))\n",
        "    entropy = 0.0\n",
        "    for group in groups:\n",
        "        size = float(len(group))\n",
        "        if size == 0:\n",
        "            continue\n",
        "        score = 0.0\n",
        "        for class_val in classes:\n",
        "            p = [row[-1] == class_val for row in group].count(True) / size\n",
        "            if p > 0:\n",
        "                score -= p * np.log2(p)\n",
        "        entropy += (size / total_instances) * score\n",
        "    return entropy\n",
        "\n",
        "def test_split(index, value, dataset):\n",
        "    left, right = [], []\n",
        "    for row in dataset:\n",
        "        if row[index] < value:\n",
        "            left.append(row)\n",
        "        else:\n",
        "            right.append(row)\n",
        "    return left, right\n",
        "\n",
        "def get_split(dataset):\n",
        "    class_values = list(set(row[-1] for row in dataset))\n",
        "    b_index, b_value, b_score, b_groups = 999, 999, float('inf'), None\n",
        "    for index in range(len(dataset[0])-1):\n",
        "        for row in dataset:\n",
        "            groups = test_split(index, row[index], dataset)\n",
        "            ent = entropy(groups, class_values)\n",
        "            if ent < b_score:\n",
        "                b_index, b_value, b_score, b_groups = index, row[index], ent, groups\n",
        "    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n",
        "\n",
        "def to_terminal(group):\n",
        "    outcomes = [row[-1] for row in group]\n",
        "    return max(set(outcomes), key=outcomes.count)\n",
        "\n",
        "def split(node, max_depth, min_size, depth):\n",
        "    left, right = node['groups']\n",
        "    del(node['groups'])\n",
        "    if not left or not right:\n",
        "        node['left'] = node['right'] = to_terminal(left + right)\n",
        "        return\n",
        "    if depth >= max_depth:\n",
        "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
        "        return\n",
        "    if len(left) <= min_size:\n",
        "        node['left'] = to_terminal(left)\n",
        "    else:\n",
        "        node['left'] = get_split(left)\n",
        "        split(node['left'], max_depth, min_size, depth+1)\n",
        "    if len(right) <= min_size:\n",
        "        node['right'] = to_terminal(right)\n",
        "    else:\n",
        "        node['right'] = get_split(right)\n",
        "        split(node['right'], max_depth, min_size, depth+1)\n",
        "\n",
        "def build_tree(train, max_depth, min_size):\n",
        "    root = get_split(train)\n",
        "    split(root, max_depth, min_size, 1)\n",
        "    return root\n",
        "\n",
        "def predict(node, row):\n",
        "    if row[node['index']] < node['value']:\n",
        "        if isinstance(node['left'], dict):\n",
        "            return predict(node['left'], row)\n",
        "        else:\n",
        "            return node['left']\n",
        "    else:\n",
        "        if isinstance(node['right'], dict):\n",
        "            return predict(node['right'], row)\n",
        "        else:\n",
        "            return node['right']\n",
        "\n",
        "def calculate_metrics(conf_matrix):\n",
        "    FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix)\n",
        "    FN = conf_matrix.sum(axis=1) - np.diag(conf_matrix)\n",
        "    TP = np.diag(conf_matrix)\n",
        "    TN = conf_matrix.sum() - (FP + FN + TP)\n",
        "\n",
        "    precision = np.mean(TP / (TP + FP + np.finfo(float).eps))\n",
        "    recall = np.mean(TP / (TP + FN + np.finfo(float).eps))\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + np.finfo(float).eps)\n",
        "    specificity = np.mean(TN / (TN + FP + np.finfo(float).eps))\n",
        "\n",
        "    return precision, recall, f1_score, specificity\n",
        "\n",
        "def create_confusion_matrix(actual, predicted, classes):\n",
        "    class_dict = {cls: i for i, cls in enumerate(classes)}\n",
        "    matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
        "    for a, p in zip(actual, predicted):\n",
        "        matrix[class_dict[a]][class_dict[p]] += 1\n",
        "    return matrix\n",
        "\n",
        "train = train_data.values.tolist()\n",
        "test = test_data.values.tolist()\n",
        "\n",
        "tree = build_tree(train, max_depth=3, min_size=10)\n",
        "\n",
        "predictions = [predict(tree, row) for row in test]\n",
        "\n",
        "actual = [row[-1] for row in test]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuwvy5Abn78s"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp4d4ytsn-M3",
        "outputId": "c95e2cad-9b5a-4ec8-c9fd-69d76707a23d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9\n",
            "Precision: 0.923076923076923\n",
            "Recall: 0.9\n",
            "F1-Score: 0.911392405063291\n",
            "Specificity: 0.9500000000000001\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 3  0  7]]\n"
          ]
        }
      ],
      "source": [
        "train_values = train_data.values\n",
        "test_values = test_data.values\n",
        "train_normalized = min_max_normalize(train_values[:, :-1])\n",
        "test_normalized = min_max_normalize(test_values[:, :-1])\n",
        "\n",
        "train = np.column_stack((train_normalized, train_values[:, -1]))\n",
        "test = np.column_stack((test_normalized, test_values[:, -1]))\n",
        "\n",
        "tree = build_tree(train.tolist(), max_depth=3, min_size=10)\n",
        "\n",
        "predictions = [predict(tree, row[:-1]) for row in test.tolist()]\n",
        "\n",
        "actual = [row[-1] for row in test.tolist()]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9OSM-Tznv67"
      },
      "source": [
        "## KNN\n",
        "K-Nearest Neighbor (KNN) adalah salah satu algoritma pembelajaran mesin yang paling sederhana dan banyak digunakan untuk masalah klasifikasi dan regresi. Algoritma ini bekerja berdasarkan prinsip 'kesamaan fitur' (feature similarity), yang artinya, objek baru diklasifikasikan berdasarkan mayoritas dari K objek terdekatnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHViFa1li7Gi"
      },
      "source": [
        "### A. Untuk K = 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAQ_Wwu8gkHb"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXcCl0vMjdCW",
        "outputId": "321c9a31-1051-48ac-a02f-afe140c6d766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9696969696969697\n",
            "Recall: 0.9666666666666667\n",
            "F1-Score: 0.9681794470526863\n",
            "Specificity: 0.9833333333333334\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 1  0  9]]\n"
          ]
        }
      ],
      "source": [
        "def euclidean_distance(row1, row2):\n",
        "    return np.sqrt(np.sum((np.array(row1) - np.array(row2)) ** 2))\n",
        "\n",
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "    distances = []\n",
        "    for train_row in train:\n",
        "        dist = euclidean_distance(test_row[:-1], train_row[:-1])\n",
        "        distances.append((train_row, dist))\n",
        "    distances.sort(key=lambda tup: tup[1])\n",
        "    neighbors = [distances[i][0] for i in range(num_neighbors)]\n",
        "    return neighbors\n",
        "\n",
        "def predict_classification(train, test_row, num_neighbors):\n",
        "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
        "    output_values = [row[-1] for row in neighbors]\n",
        "    prediction = max(set(output_values), key=output_values.count)\n",
        "    return prediction\n",
        "\n",
        "def create_confusion_matrix(actual, predicted, classes):\n",
        "    class_dict = {cls: i for i, cls in enumerate(classes)}\n",
        "    matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
        "    for a, p in zip(actual, predicted):\n",
        "        matrix[class_dict[a]][class_dict[p]] += 1\n",
        "    return matrix\n",
        "\n",
        "def calculate_metrics(conf_matrix):\n",
        "    FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix)\n",
        "    FN = conf_matrix.sum(axis=1) - np.diag(conf_matrix)\n",
        "    TP = np.diag(conf_matrix)\n",
        "    TN = conf_matrix.sum() - (FP + FN + TP)\n",
        "    precision = np.mean(TP / (TP + FP + np.finfo(float).eps))\n",
        "    recall = np.mean(TP / (TP + FN + np.finfo(float).eps))\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + np.finfo(float).eps)\n",
        "    specificity = np.mean(TN / (TN + FP + np.finfo(float).eps))\n",
        "    return precision, recall, f1_score, specificity\n",
        "\n",
        "train = train_data.values\n",
        "test = test_data.values\n",
        "\n",
        "num_neighbors = 3\n",
        "predictions = [predict_classification(train, row, num_neighbors) for row in test]\n",
        "\n",
        "actual = [row[-1] for row in test]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED_JIybXjavW"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFNuAxvjj00E",
        "outputId": "860289e7-7961-48a7-97ba-a35076be99c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9696969696969697\n",
            "Recall: 0.9666666666666667\n",
            "F1-Score: 0.9681794470526863\n",
            "Specificity: 0.9833333333333334\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 1  0  9]]\n"
          ]
        }
      ],
      "source": [
        "def min_max_normalize(dataset):\n",
        "    min_val = np.min(dataset, axis=0)\n",
        "    max_val = np.max(dataset, axis=0)\n",
        "    return (dataset - min_val) / (max_val - min_val + np.finfo(float).eps)\n",
        "\n",
        "train_values = train_data.values\n",
        "test_values = test_data.values\n",
        "train_normalized = min_max_normalize(train_values[:, :-1])\n",
        "train = np.column_stack((train_normalized, train_values[:, -1]))\n",
        "test_normalized = min_max_normalize(test_values[:, :-1])\n",
        "test = np.column_stack((test_normalized, test_values[:, -1]))\n",
        "\n",
        "num_neighbors = 3\n",
        "predictions = [predict_classification(train, row, num_neighbors) for row in test]\n",
        "\n",
        "actual = [row[-1] for row in test]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSYooKyIjKeF"
      },
      "source": [
        "### B. Untuk K = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5p8ZU_KjXh2"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BrgeCiYiSlj",
        "outputId": "9bf47f4d-2e67-4bd6-ba0d-e6f673fd50cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9696969696969697\n",
            "Recall: 0.9666666666666667\n",
            "F1-Score: 0.9681794470526863\n",
            "Specificity: 0.9833333333333334\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 1  0  9]]\n"
          ]
        }
      ],
      "source": [
        "train = train_data.values\n",
        "test = test_data.values\n",
        "\n",
        "num_neighbors = 5\n",
        "predictions = [predict_classification(train, row, num_neighbors) for row in test]\n",
        "\n",
        "actual = [row[-1] for row in test]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGm5405agnUB"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU4_LlLNgqUX",
        "outputId": "398511f0-8ec3-4da7-f8c4-e3ab502ed63e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9696969696969697\n",
            "Recall: 0.9666666666666667\n",
            "F1-Score: 0.9681794470526863\n",
            "Specificity: 0.9833333333333334\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 1  0  9]]\n"
          ]
        }
      ],
      "source": [
        "train_values = train_data.values\n",
        "test_values = test_data.values\n",
        "train_normalized = min_max_normalize(train_values[:, :-1])\n",
        "train = np.column_stack((train_normalized, train_values[:, -1]))\n",
        "test_normalized = min_max_normalize(test_values[:, :-1])\n",
        "test = np.column_stack((test_normalized, test_values[:, -1]))\n",
        "\n",
        "num_neighbors = 5\n",
        "predictions = [predict_classification(train, row, num_neighbors) for row in test]\n",
        "\n",
        "actual = [row[-1] for row in test]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS6-m9DFjRjK"
      },
      "source": [
        "### C. Untuk K = 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHhsGY0ik5Px"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMNRmzurlBXe",
        "outputId": "3b8ae89d-b47a-4017-9d43-cf22696331f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 0  0 10]]\n"
          ]
        }
      ],
      "source": [
        "train = train_data.values\n",
        "test = test_data.values\n",
        "\n",
        "num_neighbors = 7\n",
        "predictions = [predict_classification(train, row, num_neighbors) for row in test]\n",
        "\n",
        "actual = [row[-1] for row in test]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buQYmzO7k818"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cT_6pMilF4w",
        "outputId": "dd7b2911-3041-41c2-b004-5900e15847fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9666666666666667\n",
            "Precision: 0.9696969696969697\n",
            "Recall: 0.9666666666666667\n",
            "F1-Score: 0.9681794470526863\n",
            "Specificity: 0.9833333333333334\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0 10  0]\n",
            " [ 1  0  9]]\n"
          ]
        }
      ],
      "source": [
        "train_values = train_data.values\n",
        "test_values = test_data.values\n",
        "train_normalized = min_max_normalize(train_values[:, :-1])\n",
        "train = np.column_stack((train_normalized, train_values[:, -1]))\n",
        "test_normalized = min_max_normalize(test_values[:, :-1])\n",
        "test = np.column_stack((test_normalized, test_values[:, -1]))\n",
        "\n",
        "num_neighbors = 7\n",
        "predictions = [predict_classification(train, row, num_neighbors) for row in test]\n",
        "\n",
        "actual = [row[-1] for row in test]\n",
        "\n",
        "classes = list(set(actual))\n",
        "conf_matrix = create_confusion_matrix(actual, predictions, classes)\n",
        "\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = sum(1 for i in range(len(predictions)) if predictions[i] == actual[i]) / float(len(actual))\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdDZ4HeBpSRA"
      },
      "source": [
        "## Naive-Bayes\n",
        "Naive Bayes merupakan teknik klasifikasi statistik yang berdasarkan pada Teorema Bayes. Ini adalah keluarga dari algoritma klasifikasi yang sederhana namun sangat efisien dan berfungsi dengan baik dengan dataset besar. Meskipun sederhana, sering kali cukup efektif dan digunakan secara luas karena kecepatan dan skalabilitasnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO_siYwspD5z"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTcwrvUKpVdd",
        "outputId": "f7b9da73-ceb2-4390-f32f-b0566b972327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9555555555555556\n",
            "Precision: 0.9444444444444445\n",
            "Recall: 0.9333333333333332\n",
            "F1-Score: 0.9326599326599326\n",
            "Specificity: 0.9666666666666667\n",
            "Confusion Matrix:\n",
            "                  Iris-setosa  Iris-versicolor  Iris-virginica\n",
            "Iris-setosa               10                0               0\n",
            "Iris-versicolor            0               10               0\n",
            "Iris-virginica             0                2               8\n"
          ]
        }
      ],
      "source": [
        "def calculate_prior(labels):\n",
        "    classes = np.unique(labels)\n",
        "    prior = {}\n",
        "    for cls in classes:\n",
        "        prior[cls] = len(labels[labels == cls]) / len(labels)\n",
        "    return prior\n",
        "\n",
        "def calculate_likelihood(features, label, cls):\n",
        "    feature_values = features[label == cls]\n",
        "    mean = np.mean(feature_values, axis=0)\n",
        "    var = np.var(feature_values, axis=0)\n",
        "    return mean, var\n",
        "\n",
        "def fit_naive_bayes(features, labels):\n",
        "    classes = np.unique(labels)\n",
        "    model = {}\n",
        "    for cls in classes:\n",
        "        model[cls] = {}\n",
        "        model[cls]['prior'] = calculate_prior(labels)[cls]\n",
        "        mean, var = calculate_likelihood(features, labels, cls)\n",
        "        model[cls]['mean'] = mean\n",
        "        model[cls]['var'] = var\n",
        "    return model\n",
        "\n",
        "def predict_single(model, input_features):\n",
        "    probabilities = {}\n",
        "    for cls, parameters in model.items():\n",
        "        total_features = len(parameters['mean'])\n",
        "        probabilities[cls] = np.log(parameters['prior'])\n",
        "        for index in range(total_features):\n",
        "            mean = parameters['mean'][index]\n",
        "            var = parameters['var'][index]\n",
        "            x = input_features[index]\n",
        "            probabilities[cls] += (-0.5 * np.log(2 * np.pi * var)) - ((x - mean) ** 2 / (2 * var))\n",
        "    return max(probabilities, key=probabilities.get)\n",
        "\n",
        "def predict(model, features):\n",
        "    predictions = []\n",
        "    for feature_set in features:\n",
        "        predictions.append(predict_single(model, feature_set))\n",
        "    return predictions\n",
        "\n",
        "train_features = train_data.iloc[:, :-1].values\n",
        "train_labels = train_data.iloc[:, -1].values\n",
        "test_features = test_data.iloc[:, :-1].values\n",
        "test_labels = test_data.iloc[:, -1].values\n",
        "\n",
        "model = fit_naive_bayes(train_features, train_labels)\n",
        "\n",
        "test_predictions = predict(model, test_features)\n",
        "\n",
        "def confusion_matrix_manual(labels, predictions, classes):\n",
        "    matrix = pd.DataFrame(np.zeros((len(classes), len(classes)), dtype=int), index=classes, columns=classes)\n",
        "    for true, pred in zip(labels, predictions):\n",
        "        matrix.loc[true, pred] += 1\n",
        "    return matrix\n",
        "\n",
        "conf_matrix = confusion_matrix_manual(test_labels, test_predictions, np.unique(test_labels))\n",
        "\n",
        "def calculate_metrics(conf_matrix):\n",
        "    metrics = {}\n",
        "    for cls in conf_matrix.columns:\n",
        "        tp = conf_matrix.loc[cls, cls]\n",
        "        fn = conf_matrix.loc[cls].sum() - tp\n",
        "        fp = conf_matrix[cls].sum() - tp\n",
        "        tn = conf_matrix.sum().sum() - tp - fp - fn\n",
        "        precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
        "        recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
        "        specificity = tn / (tn + fp) if tn + fp != 0 else 0\n",
        "        accuracy = (tp + tn) / conf_matrix.sum().sum()\n",
        "        metrics[cls] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1_score,\n",
        "            'specificity': specificity,\n",
        "            'accuracy': accuracy\n",
        "        }\n",
        "    return metrics\n",
        "\n",
        "metrics = calculate_metrics(conf_matrix)\n",
        "\n",
        "print(\"Accuracy:\", np.mean([m['accuracy'] for m in metrics.values()]))\n",
        "print(\"Precision:\", np.mean([m['precision'] for m in metrics.values()]))\n",
        "print(\"Recall:\", np.mean([m['recall'] for m in metrics.values()]))\n",
        "print(\"F1-Score:\", np.mean([m['f1_score'] for m in metrics.values()]))\n",
        "print(\"Specificity:\", np.mean([m['specificity'] for m in metrics.values()]))\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NizIT3upGkr"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Rone2rwp1LJ",
        "outputId": "35f4557d-6668-416b-e831-aac113a4275a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9777777777777779\n",
            "Precision: 0.9696969696969697\n",
            "Recall: 0.9666666666666667\n",
            "F1-Score: 0.9665831244778613\n",
            "Specificity: 0.9833333333333334\n",
            "Confusion Matrix:\n",
            "                  Iris-setosa  Iris-versicolor  Iris-virginica\n",
            "Iris-setosa               10                0               0\n",
            "Iris-versicolor            0               10               0\n",
            "Iris-virginica             0                1               9\n"
          ]
        }
      ],
      "source": [
        "def min_max_scaling(features):\n",
        "    min_vals = np.min(features, axis=0)\n",
        "    max_vals = np.max(features, axis=0)\n",
        "    return (features - min_vals) / (max_vals - min_vals)\n",
        "\n",
        "train_features = train_data.iloc[:, :-1].values\n",
        "test_features = test_data.iloc[:, :-1].values\n",
        "\n",
        "train_features_scaled = min_max_scaling(train_features)\n",
        "test_features_scaled = min_max_scaling(test_features)\n",
        "\n",
        "train_labels = train_data.iloc[:, -1].values\n",
        "test_labels = test_data.iloc[:, -1].values\n",
        "\n",
        "model = fit_naive_bayes(train_features_scaled, train_labels)\n",
        "test_predictions = predict(model, test_features_scaled)\n",
        "\n",
        "conf_matrix = confusion_matrix_manual(test_labels, test_predictions, np.unique(test_labels))\n",
        "\n",
        "metrics = calculate_metrics(conf_matrix)\n",
        "\n",
        "print(\"Accuracy:\", np.mean([m['accuracy'] for m in metrics.values()]))\n",
        "print(\"Precision:\", np.mean([m['precision'] for m in metrics.values()]))\n",
        "print(\"Recall:\", np.mean([m['recall'] for m in metrics.values()]))\n",
        "print(\"F1-Score:\", np.mean([m['f1_score'] for m in metrics.values()]))\n",
        "print(\"Specificity:\", np.mean([m['specificity'] for m in metrics.values()]))\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIJMtJuir5IJ"
      },
      "source": [
        "## Support Vector Machine (SVM)\n",
        "\n",
        "Support Vector Machine (SVM) adalah salah satu algoritma pembelajaran mesin yang paling populer dan kuat untuk klasifikasi biner, regresi, dan tugas-tugas lain yang serupa. SVM dirancang untuk menemukan hyperplane dalam ruang N-dimensi (N adalah jumlah fitur) yang secara jelas mengklasifikasikan data titik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkEC2T62q3Ct"
      },
      "source": [
        "### A. Kernel Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifsY69IKrWpu"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em_PHR4NsQI1",
        "outputId": "303f88b3-77b6-4597-d9d7-ea540f353861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            " [[10  0]\n",
            " [ 0 20]]\n"
          ]
        }
      ],
      "source": [
        "def load_data():\n",
        "    train_data = pd.read_csv('https://raw.githubusercontent.com/DaffaZimraan/ML-Classification/main/IRIS_Train.csv')\n",
        "    test_data = pd.read_csv('https://raw.githubusercontent.com/DaffaZimraan/ML-Classification/main/IRIS_Test.csv')\n",
        "    return train_data, test_data\n",
        "\n",
        "def preprocess_data(data):\n",
        "    mapping = {species: idx for idx, species in enumerate(data['species'].unique())}\n",
        "    data['species'] = data['species'].map(mapping)\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data['species'].values\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "    return X, y\n",
        "\n",
        "def train_svm(X, y, learning_rate=0.001, lambda_param=0.01, epochs=1000):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    n = len(y)\n",
        "    for _ in range(epochs):\n",
        "        for idx, x_i in enumerate(X):\n",
        "            condition = y[idx] * (np.dot(x_i, w) - b) >= 1\n",
        "            if condition:\n",
        "                w -= learning_rate * (2 * lambda_param * w)\n",
        "            else:\n",
        "                w -= learning_rate * (2 * lambda_param * w - np.dot(x_i, y[idx]))\n",
        "                b -= learning_rate * y[idx]\n",
        "    return w, b\n",
        "\n",
        "def predict(X, w, b):\n",
        "    return np.sign(np.dot(X, w) - b)\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    tp = np.sum((y_pred == 1) & (y_true == 1))\n",
        "    tn = np.sum((y_pred == -1) & (y_true == -1))\n",
        "    fp = np.sum((y_pred == 1) & (y_true == -1))\n",
        "    fn = np.sum((y_pred == -1) & (y_true == 1))\n",
        "\n",
        "    precision = tp / (tp + fp + np.finfo(float).eps)\n",
        "    recall = tp / (tp + fn + np.finfo(float).eps)\n",
        "    f1_score = 2 * precision * recall / (precision + recall + np.finfo(float).eps)\n",
        "    specificity = tn / (tn + fp + np.finfo(float).eps)\n",
        "\n",
        "    return precision, recall, f1_score, specificity\n",
        "\n",
        "def create_confusion_matrix(y_true, y_pred):\n",
        "    cm = np.zeros((2, 2), dtype=int)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[int(t == 1), int(p == 1)] += 1\n",
        "    return cm\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_test, y_test = preprocess_data(test_data)\n",
        "\n",
        "weights, bias = train_svm(X_train, y_train)\n",
        "\n",
        "predictions = predict(X_test, weights, bias)\n",
        "\n",
        "accuracy = np.mean(y_test == predictions)\n",
        "precision, recall, f1_score, specificity = calculate_metrics(y_test, predictions)\n",
        "conf_matrix = create_confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71FyI85bra5R"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSi0MoQOucXO",
        "outputId": "d45c90dd-fdf9-47c9-a64f-65bbd942a31c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            " [[10  0]\n",
            " [ 0 20]]\n"
          ]
        }
      ],
      "source": [
        "def min_max_normalize(X):\n",
        "    min_val = np.min(X, axis=0)\n",
        "    max_val = np.max(X, axis=0)\n",
        "    return (X - min_val) / (max_val - min_val + np.finfo(float).eps)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    mapping = {species: idx for idx, species in enumerate(data['species'].unique())}\n",
        "    data['species'] = data['species'].map(mapping)\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data['species'].values\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "    X = min_max_normalize(X)\n",
        "    return X, y\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_test, y_test = preprocess_data(test_data)\n",
        "\n",
        "weights, bias = train_svm(X_train, y_train)\n",
        "\n",
        "predictions = predict(X_test, weights, bias)\n",
        "\n",
        "accuracy = np.mean(y_test == predictions)\n",
        "precision, recall, f1_score, specificity = calculate_metrics(y_test, predictions)\n",
        "conf_matrix = create_confusion_matrix(y_test, predictions)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1_score)\n",
        "print(\"Specificity:\", specificity)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGJozPKUq4Ha"
      },
      "source": [
        "### B. Kernel Polynomial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNFlzXkBrdOW"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbe5W9z_vUy7",
        "outputId": "8a30ea4f-b523-428a-8cf7-403eb50cf68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 20]]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_data(data):\n",
        "    mapping = {species: idx for idx, species in enumerate(data['species'].unique())}\n",
        "    data['species'] = data['species'].map(mapping)\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data['species'].values\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "    return X, y\n",
        "\n",
        "def polynomial_kernel(x, y, degree=3, coef=1):\n",
        "    return (np.dot(x, y) + coef) ** degree\n",
        "\n",
        "def train_svm(X, y, degree=3, coef=1, learning_rate=0.001, lambda_param=0.01, epochs=500):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    for _ in range(epochs):\n",
        "        for idx, x_i in enumerate(X):\n",
        "            if y[idx] * (polynomial_kernel(x_i, w, degree, coef) + b) < 1:\n",
        "                w += learning_rate * ((y[idx] * x_i) - (2 * lambda_param * w))\n",
        "                b += learning_rate * y[idx]\n",
        "            else:\n",
        "                w -= learning_rate * (2 * lambda_param * w)\n",
        "    return w, b\n",
        "\n",
        "def predict(X, w, b, degree=3, coef=1):\n",
        "    return np.sign(np.array([polynomial_kernel(x, w, degree, coef) for x in X]) + b)\n",
        "\n",
        "def create_confusion_matrix(y_true, y_pred):\n",
        "    classes = np.unique(y_true)\n",
        "    cm = np.zeros((len(classes), len(classes)), int)\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        cm[int(true == 1), int(pred == 1)] += 1\n",
        "    return cm\n",
        "\n",
        "def calculate_metrics(cm):\n",
        "    tp = cm[1, 1]\n",
        "    tn = cm[0, 0]\n",
        "    fp = cm[0, 1]\n",
        "    fn = cm[1, 0]\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    specificity = tn / (tn + fp)\n",
        "    return precision, recall, f1_score, specificity\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_test, y_test = preprocess_data(test_data)\n",
        "\n",
        "w, b = train_svm(X_train, y_train)\n",
        "predictions = predict(X_test, w, b)\n",
        "\n",
        "conf_matrix = create_confusion_matrix(y_test, predictions)\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1_score}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oh6pX9jDreFd"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqlPt6VTvVc2",
        "outputId": "5b52a675-39d7-4f3a-eb76-a60a3ef43c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 20]]\n"
          ]
        }
      ],
      "source": [
        "def min_max_normalize(X):\n",
        "    min_val = np.min(X, axis=0)\n",
        "    max_val = np.max(X, axis=0)\n",
        "    return (X - min_val) / (max_val - min_val + np.finfo(float).eps)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    mapping = {species: idx for idx, species in enumerate(data['species'].unique())}\n",
        "    data['species'] = data['species'].map(mapping)\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data['species'].values\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "    X = min_max_normalize(X)\n",
        "    return X, y\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_test, y_test = preprocess_data(test_data)\n",
        "\n",
        "w, b = train_svm(X_train, y_train)\n",
        "predictions = predict(X_test, w, b)\n",
        "\n",
        "conf_matrix = create_confusion_matrix(y_test, predictions)\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1_score}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCbl4Su0rDzO"
      },
      "source": [
        "### C. Kernel RBF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "673nFYYGrfox"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK8OyyQkxJNU",
        "outputId": "9e127dcb-d27b-4a6e-8aa6-3b23e8324d84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 20]]\n"
          ]
        }
      ],
      "source": [
        "def min_max_normalize(X):\n",
        "    min_val = np.min(X, axis=0)\n",
        "    max_val = np.max(X, axis=0)\n",
        "    return (X - min_val) / (max_val - min_val + np.finfo(float).eps)\n",
        "\n",
        "def preprocess_data(data, normalize=False):\n",
        "    mapping = {species: idx for idx, species in enumerate(data['species'].unique())}\n",
        "    data['species'] = data['species'].map(mapping)\n",
        "    X = data.iloc[:, :-1].values\n",
        "    y = data['species'].values\n",
        "    y = np.where(y == 0, -1, 1)\n",
        "    if normalize:\n",
        "        X = min_max_normalize(X)\n",
        "    return X, y\n",
        "\n",
        "def rbf_kernel(x, y, gamma=1):\n",
        "    return np.exp(-gamma * np.linalg.norm(x - y)**2)\n",
        "\n",
        "def train_svm(X, y, kernel, gamma=1, learning_rate=0.001, lambda_param=0.01, epochs=500):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    for _ in range(epochs):\n",
        "        for idx, x_i in enumerate(X):\n",
        "            if y[idx] * (np.dot(w, x_i) + b) < 1:\n",
        "                w += learning_rate * (y[idx] * x_i - 2 * lambda_param * w)\n",
        "            else:\n",
        "                w -= learning_rate * (2 * lambda_param * w)\n",
        "    return w, b\n",
        "\n",
        "def predict(X, w, b):\n",
        "    return np.sign(np.dot(X, w) + b)\n",
        "\n",
        "def create_confusion_matrix(y_true, y_pred):\n",
        "    classes = np.unique(y_true)\n",
        "    cm = np.zeros((len(classes), len(classes)), int)\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        cm[int(true == 1), int(pred == 1)] += 1\n",
        "    return cm\n",
        "\n",
        "def calculate_metrics(cm):\n",
        "    tp = cm[1, 1]\n",
        "    tn = cm[0, 0]\n",
        "    fp = cm[0, 1]\n",
        "    fn = cm[1, 0]\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    specificity = tn / (tn + fp)\n",
        "    return precision, recall, f1_score, specificity\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "X_train, y_train = preprocess_data(train_data, normalize=False)\n",
        "X_test, y_test = preprocess_data(test_data, normalize=False)\n",
        "\n",
        "w, b = train_svm(X_train, y_train, rbf_kernel)\n",
        "predictions = predict(X_test, w, b)\n",
        "\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "conf_matrix = create_confusion_matrix(y_test, predictions)\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1_score}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONJtGOYMrgMd"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pRZEqNYxDCM",
        "outputId": "8bebff8d-62da-470f-bbc0-de0322a04b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 20]]\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = load_data()\n",
        "X_train, y_train = preprocess_data(train_data, normalize=True)\n",
        "X_test, y_test = preprocess_data(test_data, normalize=True)\n",
        "\n",
        "w, b = train_svm(X_train, y_train, rbf_kernel)\n",
        "predictions = predict(X_test, w, b)\n",
        "\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "conf_matrix = create_confusion_matrix(y_test, predictions)\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1_score}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCWi3WiorRu3"
      },
      "source": [
        "### D. Kernel Sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb3WaeOsriBB"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJtkKE36yamr",
        "outputId": "7c755edf-1553-4175-f801-ac5abb52a472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 20]]\n"
          ]
        }
      ],
      "source": [
        "def sigmoid_kernel(x, y, gamma=0.1, coef0=1):\n",
        "    return np.tanh(gamma * np.dot(x, y) + coef0)\n",
        "\n",
        "def train_svm(X, y, kernel=sigmoid_kernel, gamma=0.1, coef0=1, learning_rate=0.001, lambda_param=0.01, epochs=500):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    for _ in range(epochs):\n",
        "        for idx, x_i in enumerate(X):\n",
        "            condition = y[idx] * (np.dot(w, x_i) + b) < 1\n",
        "            if condition:\n",
        "                w += learning_rate * (y[idx] * x_i - 2 * lambda_param * w)\n",
        "            else:\n",
        "                w -= learning_rate * (2 * lambda_param * w)\n",
        "    return w, b\n",
        "\n",
        "def predict(X, w, b):\n",
        "    return np.sign(np.dot(X, w) + b)\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "X_train, y_train = preprocess_data(train_data, normalize=False)\n",
        "X_test, y_test = preprocess_data(test_data, normalize=False)\n",
        "\n",
        "w, b = train_svm(X_train, y_train, kernel=sigmoid_kernel)\n",
        "predictions = predict(X_test, w, b)\n",
        "\n",
        "conf_matrix = create_confusion_matrix(y_test, predictions)\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall:}\")\n",
        "print(f\"F1-Score: {f1_score}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaxu-9usrjCp"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTh1_UVzy316",
        "outputId": "bb5460fd-caa1-462f-83dc-4b8f396efe7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-Score: 1.0\n",
            "Specificity: 1.0\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 20]]\n"
          ]
        }
      ],
      "source": [
        "def sigmoid_kernel(x, y, gamma=0.1, coef0=1):\n",
        "    return np.tanh(gamma * np.dot(x, y) + coef0)\n",
        "\n",
        "def train_svm(X, y, kernel=sigmoid_kernel, gamma=0.1, coef0=1, learning_rate=0.001, lambda_param=0.01, epochs=500):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0\n",
        "    for _ in range(epochs):\n",
        "        for idx, x_i in enumerate(X):\n",
        "            condition = y[idx] * (np.dot(w, x_i) + b) < 1\n",
        "            if condition:\n",
        "                w += learning_rate * (y[idx] * x_i - 2 * lambda_param * w)\n",
        "            else:\n",
        "                w -= learning_rate * (2 * lambda_param * w)\n",
        "    return w, b\n",
        "\n",
        "def predict(X, w, b):\n",
        "    return np.sign(np.dot(X, w) + b)\n",
        "\n",
        "train_data, test_data = load_data()\n",
        "X_train, y_train = preprocess_data(train_data, normalize=True)\n",
        "X_test, y_test = preprocess_data(test_data, normalize=True)\n",
        "\n",
        "w, b = train_svm(X_train, y_train, kernel=sigmoid_kernel)\n",
        "predictions = predict(X_test, w, b)\n",
        "\n",
        "conf_matrix = create_confusion_matrix(y_test, predictions)\n",
        "precision, recall, f1_score, specificity = calculate_metrics(conf_matrix)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall:}\")\n",
        "print(f\"F1-Score: {f1_score}\")\n",
        "print(f\"Specificity: {specificity}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk2Ck9jgrgCC"
      },
      "source": [
        "## Artificial Neural Network (ANN)\n",
        "\n",
        "ANN terdiri dari lapisan input, satu atau lebih lapisan tersembunyi, dan lapisan output. Setiap neuron dalam satu lapisan terhubung dengan neuron di lapisan berikutnya melalui bobot, yang merupakan parameter yang disesuaikan selama proses pembelajaran. Neuron ini mengumpulkan input, menerapkannya pada suatu fungsi (biasanya non-linear), dan mengirimkan output ke neuron berikutnya."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4wBePXf0uXg"
      },
      "source": [
        "### Tanpa Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqH6Y0mimBb4",
        "outputId": "024cc21f-dc28-48e1-a7f4-d855839b9888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 1s 6ms/step - loss: 1.3300 - accuracy: 0.3500 - precision_3: 0.4348 - recall_3: 0.3333\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 1.2518 - accuracy: 0.3917 - precision_3: 0.4762 - recall_3: 0.3333\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 1.1913 - accuracy: 0.4833 - precision_3: 0.4938 - recall_3: 0.3333\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.1344 - accuracy: 0.5750 - precision_3: 0.5000 - recall_3: 0.3333\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.0874 - accuracy: 0.6250 - precision_3: 0.5181 - recall_3: 0.3583\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.0402 - accuracy: 0.6500 - precision_3: 0.5402 - recall_3: 0.3917\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.0069 - accuracy: 0.6667 - precision_3: 0.5556 - recall_3: 0.4167\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9738 - accuracy: 0.6667 - precision_3: 0.5918 - recall_3: 0.4833\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9471 - accuracy: 0.6667 - precision_3: 0.6117 - recall_3: 0.5250\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9236 - accuracy: 0.6667 - precision_3: 0.6226 - recall_3: 0.5500\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.9014 - accuracy: 0.6667 - precision_3: 0.6460 - recall_3: 0.6083\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8839 - accuracy: 0.6667 - precision_3: 0.6552 - recall_3: 0.6333\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.8657 - accuracy: 0.6667 - precision_3: 0.6581 - recall_3: 0.6417\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.8477 - accuracy: 0.6667 - precision_3: 0.6610 - recall_3: 0.6500\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8313 - accuracy: 0.6667 - precision_3: 0.6610 - recall_3: 0.6500\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.8154 - accuracy: 0.6667 - precision_3: 0.6639 - recall_3: 0.6583\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.8030 - accuracy: 0.6667 - precision_3: 0.6639 - recall_3: 0.6583\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7890 - accuracy: 0.6667 - precision_3: 0.6610 - recall_3: 0.6500\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7746 - accuracy: 0.6667 - precision_3: 0.6581 - recall_3: 0.6417\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7614 - accuracy: 0.6667 - precision_3: 0.6581 - recall_3: 0.6417\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7492 - accuracy: 0.6667 - precision_3: 0.6581 - recall_3: 0.6417\n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7375 - accuracy: 0.6667 - precision_3: 0.6552 - recall_3: 0.6333\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7246 - accuracy: 0.6667 - precision_3: 0.6522 - recall_3: 0.6250\n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.7141 - accuracy: 0.6667 - precision_3: 0.6396 - recall_3: 0.5917\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.7024 - accuracy: 0.6667 - precision_3: 0.6330 - recall_3: 0.5750\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6916 - accuracy: 0.6667 - precision_3: 0.6226 - recall_3: 0.5500\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6817 - accuracy: 0.6667 - precision_3: 0.6000 - recall_3: 0.5000\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6709 - accuracy: 0.6667 - precision_3: 0.5789 - recall_3: 0.4583\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6621 - accuracy: 0.6667 - precision_3: 0.5604 - recall_3: 0.4250\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6521 - accuracy: 0.6667 - precision_3: 0.5632 - recall_3: 0.4083\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6428 - accuracy: 0.6667 - precision_3: 0.5595 - recall_3: 0.3917\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6350 - accuracy: 0.6667 - precision_3: 0.5811 - recall_3: 0.3583\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6259 - accuracy: 0.6667 - precision_3: 0.6212 - recall_3: 0.3417\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6182 - accuracy: 0.6250 - precision_3: 0.6667 - recall_3: 0.3333\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6105 - accuracy: 0.6417 - precision_3: 0.7547 - recall_3: 0.3333\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.6033 - accuracy: 0.6583 - precision_3: 0.8889 - recall_3: 0.3333\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5964 - accuracy: 0.7000 - precision_3: 0.9302 - recall_3: 0.3333\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5897 - accuracy: 0.7750 - precision_3: 0.9565 - recall_3: 0.3667\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5836 - accuracy: 0.7583 - precision_3: 0.9848 - recall_3: 0.5417\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5781 - accuracy: 0.7333 - precision_3: 0.9615 - recall_3: 0.6250\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5724 - accuracy: 0.7250 - precision_3: 0.9630 - recall_3: 0.6500\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5674 - accuracy: 0.7083 - precision_3: 0.9302 - recall_3: 0.6667\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.5623 - accuracy: 0.7000 - precision_3: 0.9091 - recall_3: 0.6667\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5575 - accuracy: 0.6917 - precision_3: 0.8791 - recall_3: 0.6667\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5531 - accuracy: 0.6833 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5488 - accuracy: 0.6750 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5445 - accuracy: 0.6750 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5403 - accuracy: 0.6833 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5363 - accuracy: 0.6833 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5323 - accuracy: 0.6833 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5284 - accuracy: 0.7083 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5250 - accuracy: 0.7167 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5215 - accuracy: 0.7333 - precision_3: 0.8696 - recall_3: 0.6667\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5181 - accuracy: 0.7333 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5148 - accuracy: 0.7250 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5119 - accuracy: 0.7167 - precision_3: 0.8421 - recall_3: 0.6667\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5083 - accuracy: 0.7083 - precision_3: 0.8421 - recall_3: 0.6667\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5053 - accuracy: 0.7167 - precision_3: 0.8421 - recall_3: 0.6667\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.5025 - accuracy: 0.7167 - precision_3: 0.8421 - recall_3: 0.6667\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4995 - accuracy: 0.7417 - precision_3: 0.8421 - recall_3: 0.6667\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4968 - accuracy: 0.7500 - precision_3: 0.8511 - recall_3: 0.6667\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4942 - accuracy: 0.7500 - precision_3: 0.8421 - recall_3: 0.6667\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4918 - accuracy: 0.7583 - precision_3: 0.8511 - recall_3: 0.6667\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4889 - accuracy: 0.8167 - precision_3: 0.8602 - recall_3: 0.6667\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4863 - accuracy: 0.8167 - precision_3: 0.8632 - recall_3: 0.6833\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4840 - accuracy: 0.7917 - precision_3: 0.8351 - recall_3: 0.6750\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.7667 - precision_3: 0.8351 - recall_3: 0.6750\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4789 - accuracy: 0.7750 - precision_3: 0.8351 - recall_3: 0.6750\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4767 - accuracy: 0.7750 - precision_3: 0.8351 - recall_3: 0.6750\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4745 - accuracy: 0.7750 - precision_3: 0.8265 - recall_3: 0.6750\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4723 - accuracy: 0.7583 - precision_3: 0.8265 - recall_3: 0.6750\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4701 - accuracy: 0.7917 - precision_3: 0.8300 - recall_3: 0.6917\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4679 - accuracy: 0.8083 - precision_3: 0.8400 - recall_3: 0.7000\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4657 - accuracy: 0.8250 - precision_3: 0.8431 - recall_3: 0.7167\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4637 - accuracy: 0.8250 - precision_3: 0.8431 - recall_3: 0.7167\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4618 - accuracy: 0.8417 - precision_3: 0.8431 - recall_3: 0.7167\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.8333 - precision_3: 0.8447 - recall_3: 0.7250\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.8333 - precision_3: 0.8431 - recall_3: 0.7167\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4555 - accuracy: 0.8250 - precision_3: 0.8462 - recall_3: 0.7333\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4536 - accuracy: 0.8417 - precision_3: 0.8641 - recall_3: 0.7417\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4515 - accuracy: 0.8500 - precision_3: 0.8846 - recall_3: 0.7667\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4496 - accuracy: 0.8500 - precision_3: 0.8879 - recall_3: 0.7917\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4488 - accuracy: 0.8417 - precision_3: 0.8654 - recall_3: 0.7500\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4459 - accuracy: 0.8500 - precision_3: 0.8774 - recall_3: 0.7750\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4443 - accuracy: 0.8500 - precision_3: 0.8818 - recall_3: 0.8083\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4420 - accuracy: 0.8583 - precision_3: 0.8899 - recall_3: 0.8083\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4404 - accuracy: 0.8500 - precision_3: 0.8889 - recall_3: 0.8000\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4385 - accuracy: 0.8500 - precision_3: 0.8889 - recall_3: 0.8000\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4370 - accuracy: 0.8583 - precision_3: 0.8909 - recall_3: 0.8167\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4353 - accuracy: 0.8583 - precision_3: 0.8909 - recall_3: 0.8167\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4331 - accuracy: 0.8500 - precision_3: 0.8818 - recall_3: 0.8083\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 0.4313 - accuracy: 0.8500 - precision_3: 0.8818 - recall_3: 0.8083\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4297 - accuracy: 0.8500 - precision_3: 0.8899 - recall_3: 0.8083\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4284 - accuracy: 0.8500 - precision_3: 0.8829 - recall_3: 0.8167\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4260 - accuracy: 0.8500 - precision_3: 0.8909 - recall_3: 0.8167\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4247 - accuracy: 0.8750 - precision_3: 0.8929 - recall_3: 0.8333\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4226 - accuracy: 0.9000 - precision_3: 0.9018 - recall_3: 0.8417\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4211 - accuracy: 0.9000 - precision_3: 0.9027 - recall_3: 0.8500\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4200 - accuracy: 0.8917 - precision_3: 0.9027 - recall_3: 0.8500\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 0.4175 - accuracy: 0.9000 - precision_3: 0.8938 - recall_3: 0.8417\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "Test Loss: 0.4169, Test Accuracy: 0.9000\n",
            "Precision: 0.9286, Recall: 0.8667, F1-Score: 0.8977\n",
            "Specificity: 0.9750\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  7  3]\n",
            " [ 0  0 10]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "X_train = train_data.drop(\"species\", axis=1).values\n",
        "X_test = test_data.drop(\"species\", axis=1).values\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_data['species'])\n",
        "y_test = le.transform(test_data['species'])\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(6, input_dim=X_train.shape[1], activation='relu'),\n",
        "    Dense(6, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=1)\n",
        "\n",
        "loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix)\n",
        "TN = conf_matrix.sum() - (conf_matrix.sum(axis=0) - np.diag(conf_matrix)) - (conf_matrix.sum(axis=1) - np.diag(conf_matrix)) + np.diag(conf_matrix)\n",
        "specificity = np.mean(TN / (TN + FP + np.finfo(float).eps))\n",
        "\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
        "print(f'Specificity: {specificity:.4f}')\n",
        "print('Confusion Matrix:\\n', conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiGJecQ50zWm"
      },
      "source": [
        "### Dengan Normalisasi Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAKO43Uv02jW",
        "outputId": "633e41b4-215b-4a83-ff9b-c8288d3d55fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "4/4 [==============================] - 2s 9ms/step - loss: 1.3118 - accuracy: 0.2000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 2/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.2889 - accuracy: 0.2000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 3/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.2663 - accuracy: 0.2000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 4/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.2461 - accuracy: 0.2000 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 5/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.2256 - accuracy: 0.1917 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 6/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.2078 - accuracy: 0.2167 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 7/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.1892 - accuracy: 0.2083 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 8/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.1723 - accuracy: 0.2083 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 9/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.1565 - accuracy: 0.2083 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 10/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.1414 - accuracy: 0.2167 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 11/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.1262 - accuracy: 0.2167 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 12/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.1128 - accuracy: 0.2250 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 13/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0994 - accuracy: 0.2417 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 14/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0863 - accuracy: 0.2583 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 15/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 1.0742 - accuracy: 0.2667 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 16/100\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 1.0623 - accuracy: 0.2833 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 17/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 1.0500 - accuracy: 0.3083 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 18/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0386 - accuracy: 0.3667 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 19/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.0268 - accuracy: 0.4167 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 20/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 1.0151 - accuracy: 0.4667 - precision_2: 0.0000e+00 - recall_2: 0.0000e+00\n",
            "Epoch 21/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 1.0037 - accuracy: 0.4917 - precision_2: 0.3333 - recall_2: 0.0083   \n",
            "Epoch 22/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.9922 - accuracy: 0.5500 - precision_2: 0.3333 - recall_2: 0.0083\n",
            "Epoch 23/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.9803 - accuracy: 0.5917 - precision_2: 1.0000 - recall_2: 0.0167   \n",
            "Epoch 24/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.9681 - accuracy: 0.6083 - precision_2: 1.0000 - recall_2: 0.0333\n",
            "Epoch 25/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.9571 - accuracy: 0.6167 - precision_2: 1.0000 - recall_2: 0.0333\n",
            "Epoch 26/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9451 - accuracy: 0.6333 - precision_2: 1.0000 - recall_2: 0.0667\n",
            "Epoch 27/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9326 - accuracy: 0.6750 - precision_2: 1.0000 - recall_2: 0.1083\n",
            "Epoch 28/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.9206 - accuracy: 0.6917 - precision_2: 1.0000 - recall_2: 0.1250\n",
            "Epoch 29/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.9092 - accuracy: 0.6917 - precision_2: 1.0000 - recall_2: 0.1417\n",
            "Epoch 30/100\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.8966 - accuracy: 0.7000 - precision_2: 1.0000 - recall_2: 0.1667\n",
            "Epoch 31/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.8846 - accuracy: 0.7083 - precision_2: 1.0000 - recall_2: 0.2000\n",
            "Epoch 32/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.8724 - accuracy: 0.7083 - precision_2: 1.0000 - recall_2: 0.2083\n",
            "Epoch 33/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.8601 - accuracy: 0.7167 - precision_2: 1.0000 - recall_2: 0.2333\n",
            "Epoch 34/100\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.8474 - accuracy: 0.7083 - precision_2: 1.0000 - recall_2: 0.2667\n",
            "Epoch 35/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.8357 - accuracy: 0.7083 - precision_2: 0.9429 - recall_2: 0.2750\n",
            "Epoch 36/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.8235 - accuracy: 0.7250 - precision_2: 0.9524 - recall_2: 0.3333\n",
            "Epoch 37/100\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.8112 - accuracy: 0.7250 - precision_2: 0.9524 - recall_2: 0.3333\n",
            "Epoch 38/100\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.7991 - accuracy: 0.7250 - precision_2: 0.9184 - recall_2: 0.3750\n",
            "Epoch 39/100\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.7872 - accuracy: 0.7333 - precision_2: 0.9231 - recall_2: 0.4000\n",
            "Epoch 40/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7751 - accuracy: 0.7500 - precision_2: 0.9273 - recall_2: 0.4250\n",
            "Epoch 41/100\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.7635 - accuracy: 0.7583 - precision_2: 0.9153 - recall_2: 0.4500\n",
            "Epoch 42/100\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.7513 - accuracy: 0.7583 - precision_2: 0.9231 - recall_2: 0.5000\n",
            "Epoch 43/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7396 - accuracy: 0.7583 - precision_2: 0.9275 - recall_2: 0.5333\n",
            "Epoch 44/100\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.7283 - accuracy: 0.7667 - precision_2: 0.8933 - recall_2: 0.5583\n",
            "Epoch 45/100\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.7170 - accuracy: 0.7667 - precision_2: 0.8961 - recall_2: 0.5750\n",
            "Epoch 46/100\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.7060 - accuracy: 0.7667 - precision_2: 0.8987 - recall_2: 0.5917\n",
            "Epoch 47/100\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.6958 - accuracy: 0.7750 - precision_2: 0.8810 - recall_2: 0.6167\n",
            "Epoch 48/100\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6849 - accuracy: 0.7750 - precision_2: 0.8706 - recall_2: 0.6167\n",
            "Epoch 49/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6743 - accuracy: 0.7750 - precision_2: 0.8706 - recall_2: 0.6167\n",
            "Epoch 50/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.6640 - accuracy: 0.7750 - precision_2: 0.8706 - recall_2: 0.6167\n",
            "Epoch 51/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6543 - accuracy: 0.7750 - precision_2: 0.8706 - recall_2: 0.6167\n",
            "Epoch 52/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6449 - accuracy: 0.7750 - precision_2: 0.8706 - recall_2: 0.6167\n",
            "Epoch 53/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6352 - accuracy: 0.7750 - precision_2: 0.8706 - recall_2: 0.6167\n",
            "Epoch 54/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6260 - accuracy: 0.7833 - precision_2: 0.8706 - recall_2: 0.6167\n",
            "Epoch 55/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.6170 - accuracy: 0.7833 - precision_2: 0.8605 - recall_2: 0.6167\n",
            "Epoch 56/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6083 - accuracy: 0.7833 - precision_2: 0.8605 - recall_2: 0.6167\n",
            "Epoch 57/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7917 - precision_2: 0.8636 - recall_2: 0.6333\n",
            "Epoch 58/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5914 - accuracy: 0.7917 - precision_2: 0.8636 - recall_2: 0.6333\n",
            "Epoch 59/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5830 - accuracy: 0.7917 - precision_2: 0.8652 - recall_2: 0.6417\n",
            "Epoch 60/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5749 - accuracy: 0.8000 - precision_2: 0.8667 - recall_2: 0.6500\n",
            "Epoch 61/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5671 - accuracy: 0.8000 - precision_2: 0.8571 - recall_2: 0.6500\n",
            "Epoch 62/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5596 - accuracy: 0.8000 - precision_2: 0.8587 - recall_2: 0.6583\n",
            "Epoch 63/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5524 - accuracy: 0.8000 - precision_2: 0.8587 - recall_2: 0.6583\n",
            "Epoch 64/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5446 - accuracy: 0.8083 - precision_2: 0.8587 - recall_2: 0.6583\n",
            "Epoch 65/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5378 - accuracy: 0.8167 - precision_2: 0.8602 - recall_2: 0.6667\n",
            "Epoch 66/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5309 - accuracy: 0.8250 - precision_2: 0.8617 - recall_2: 0.6750\n",
            "Epoch 67/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5244 - accuracy: 0.8250 - precision_2: 0.8632 - recall_2: 0.6833\n",
            "Epoch 68/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.5177 - accuracy: 0.8250 - precision_2: 0.8646 - recall_2: 0.6917\n",
            "Epoch 69/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5114 - accuracy: 0.8250 - precision_2: 0.8646 - recall_2: 0.6917\n",
            "Epoch 70/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.5051 - accuracy: 0.8250 - precision_2: 0.8646 - recall_2: 0.6917\n",
            "Epoch 71/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4993 - accuracy: 0.8250 - precision_2: 0.8646 - recall_2: 0.6917\n",
            "Epoch 72/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4934 - accuracy: 0.8250 - precision_2: 0.8571 - recall_2: 0.7000\n",
            "Epoch 73/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4877 - accuracy: 0.8250 - precision_2: 0.8571 - recall_2: 0.7000\n",
            "Epoch 74/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4824 - accuracy: 0.8333 - precision_2: 0.8586 - recall_2: 0.7083\n",
            "Epoch 75/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4769 - accuracy: 0.8333 - precision_2: 0.8600 - recall_2: 0.7167\n",
            "Epoch 76/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4716 - accuracy: 0.8333 - precision_2: 0.8614 - recall_2: 0.7250\n",
            "Epoch 77/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4666 - accuracy: 0.8333 - precision_2: 0.8627 - recall_2: 0.7333\n",
            "Epoch 78/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4618 - accuracy: 0.8333 - precision_2: 0.8627 - recall_2: 0.7333\n",
            "Epoch 79/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4568 - accuracy: 0.8333 - precision_2: 0.8641 - recall_2: 0.7417\n",
            "Epoch 80/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4522 - accuracy: 0.8333 - precision_2: 0.8654 - recall_2: 0.7500\n",
            "Epoch 81/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4476 - accuracy: 0.8333 - precision_2: 0.8667 - recall_2: 0.7583\n",
            "Epoch 82/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4431 - accuracy: 0.8333 - precision_2: 0.8667 - recall_2: 0.7583\n",
            "Epoch 83/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4386 - accuracy: 0.8333 - precision_2: 0.8679 - recall_2: 0.7667\n",
            "Epoch 84/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4344 - accuracy: 0.8333 - precision_2: 0.8679 - recall_2: 0.7667\n",
            "Epoch 85/100\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4302 - accuracy: 0.8333 - precision_2: 0.8692 - recall_2: 0.7750\n",
            "Epoch 86/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4263 - accuracy: 0.8417 - precision_2: 0.8692 - recall_2: 0.7750\n",
            "Epoch 87/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4222 - accuracy: 0.8417 - precision_2: 0.8716 - recall_2: 0.7917\n",
            "Epoch 88/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4184 - accuracy: 0.8417 - precision_2: 0.8727 - recall_2: 0.8000\n",
            "Epoch 89/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4147 - accuracy: 0.8417 - precision_2: 0.8727 - recall_2: 0.8000\n",
            "Epoch 90/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4111 - accuracy: 0.8417 - precision_2: 0.8739 - recall_2: 0.8083\n",
            "Epoch 91/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4076 - accuracy: 0.8500 - precision_2: 0.8739 - recall_2: 0.8083\n",
            "Epoch 92/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.4045 - accuracy: 0.8583 - precision_2: 0.8750 - recall_2: 0.8167\n",
            "Epoch 93/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.4008 - accuracy: 0.8583 - precision_2: 0.8761 - recall_2: 0.8250\n",
            "Epoch 94/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3977 - accuracy: 0.8583 - precision_2: 0.8761 - recall_2: 0.8250\n",
            "Epoch 95/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.3945 - accuracy: 0.8583 - precision_2: 0.8783 - recall_2: 0.8417\n",
            "Epoch 96/100\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3914 - accuracy: 0.8583 - precision_2: 0.8783 - recall_2: 0.8417\n",
            "Epoch 97/100\n",
            "4/4 [==============================] - 0s 7ms/step - loss: 0.3886 - accuracy: 0.8583 - precision_2: 0.8783 - recall_2: 0.8417\n",
            "Epoch 98/100\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.3855 - accuracy: 0.8583 - precision_2: 0.8783 - recall_2: 0.8417\n",
            "Epoch 99/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3829 - accuracy: 0.8583 - precision_2: 0.8783 - recall_2: 0.8417\n",
            "Epoch 100/100\n",
            "4/4 [==============================] - 0s 6ms/step - loss: 0.3800 - accuracy: 0.8667 - precision_2: 0.8860 - recall_2: 0.8417\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Test Loss: 0.3447, Test Accuracy: 0.8667\n",
            "Precision: 0.8966, Recall: 0.8667, F1-Score: 0.8667\n",
            "Specificity: 0.9630\n",
            "Confusion Matrix:\n",
            " [[10  0  0]\n",
            " [ 0  8  2]\n",
            " [ 0  2  8]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "\n",
        "X_train = train_data.drop(\"species\", axis=1).values\n",
        "X_test = test_data.drop(\"species\", axis=1).values\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(train_data['species'])\n",
        "y_test = le.transform(test_data['species'])\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(6, input_dim=X_train.shape[1], activation='relu'),\n",
        "    Dense(6, activation='relu'),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=100, verbose=1)\n",
        "\n",
        "loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='macro')\n",
        "\n",
        "FP = conf_matrix.sum(axis=0) - np.diag(conf_matrix)\n",
        "TN = conf_matrix.sum() - (conf_matrix.sum(axis=0) - np.diag(conf_matrix)) - (conf_matrix.sum(axis=1) - np.diag(conf_matrix)) + np.diag(conf_matrix)\n",
        "specificity = np.mean(TN / (TN + FP + np.finfo(float).eps))\n",
        "\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
        "print(f'Specificity: {specificity:.4f}')\n",
        "print('Confusion Matrix:\\n', conf_matrix)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "D5XwuiBEefrk",
        "U9OSM-Tznv67",
        "kdDZ4HeBpSRA",
        "SIJMtJuir5IJ",
        "ifsY69IKrWpu",
        "71FyI85bra5R",
        "MNFlzXkBrdOW",
        "oh6pX9jDreFd",
        "673nFYYGrfox",
        "ONJtGOYMrgMd",
        "hb3WaeOsriBB",
        "iaxu-9usrjCp",
        "Pk2Ck9jgrgCC"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
